# -*- coding: utf-8 -*-
"""Web Scraping CodingBat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KTCNXlChtWxQsadFh0lsZoLFG0q06u8i
"""

!pip install fake_useragent

import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from urllib.parse import urljoin

url = 'https://codingbat.com'
file_name = 'coding_bat.txt'

user_agent = UserAgent()

response = requests.get(url, headers={'User-Agent': user_agent.chrome})
if response.status_code == 200:
    page_content = response.text
else:
    print(f"Error al acceder a la p√°gina: {response.status_code}")
    exit()

soup = BeautifulSoup(page_content, 'lxml')

all_divs = soup.find_all('div',class_='summ')

for div in all_divs:
  print(url + div.a['href'])

all_links = [url + div.a['href'] for div in all_divs]

for link in all_links:
  inner_page = requests.get(link,headers={'User-Agent': user_agent.chrome})
  inner_soup = BeautifulSoup(inner_page.content,'lxml')
  div = inner_soup.find('div',class_='tabc')
  for td in div.table.find_all('td'):
    print(url + td.a['href'])

question_links = []

for link in all_links:
    inner_page = requests.get(link, headers={'User-Agent': user_agent.chrome})
    inner_soup = BeautifulSoup(inner_page.content, 'lxml')
    div = inner_soup.find('div', class_='tabc')
    if div and div.table:
        for td in div.table.find_all('td'):
            question_links.append(url + td.a['href'])

for question_link in question_links:
  final_page = requests.get(question_link)
  final_soup = BeautifulSoup(final_page.content,'lxml')
  indent_div = final_soup.find('div',class_='indent')
  problem_statement = indent_div.table.div.string

  siblings_of_statement = indent_div.table.div.next_siblings

  examples = [sibling for sibling in siblings_of_statement if sibling.string is not None]

  print(problem_statement)
  for example in examples:
    print(example)

  print('\n\n\n')

